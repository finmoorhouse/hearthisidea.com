---
number: "52"
path: "/episodes/aird"
date: "2022-08-31"
title: "Michael Aird on how to do Impact-Driven Research"
audio: "https://pinecast.com/listen/98247b80-d49b-4f0a-9362-478b7d8d44eb.mp3"
featuredImage: "images/aird-share.png"
backgroundImage: "images/aird-bg.jpg"
apple: "https://podcasts.apple.com/gb/podcast/michael-aird-on-how-to-do-impact-driven-research/id1496501781?i=1000577939620"
spotify: "https://open.spotify.com/episode/3U9YxXBcyhnvQ8SWxoNSKf?si=oU86_Q0ITLSqdfmzRlmEpQ"
google: "https://podcasts.google.com/feed/aHR0cHM6Ly9waW5lY2FzdC5jb20vZmVlZC9oZWFyLXRoaXMtaWRlYQ/episode/aHR0cHM6Ly9waW5lY2FzdC5jb20vZ3VpZC85ODI0N2I4MC1kNDliLTRmMGEtOTM2Mi00NzhiN2Q4ZDQ0ZWI?sa=X&ved=0CAUQkfYCahcKEwjo8fDK__X5AhUAAAAAHQAAAAAQAQ"
status: "live"
---

In this episode, we talked to [Michael Aird](https://forum.effectivealtruism.org/users/michaela).

Michael is a senior research manager at [Rethink Priorities](https://www.rethinkpriorities.org/), where he co-leads the Artificial Intelligence Governance and Strategy team alongside Amanda El-Dakhakhni. Before that he conducted nuclear risk research for Rethink Priorities and longtermist macrostrategy research for Convergence Analysis, the Center on Long-Term Risk, and the Future of Humanity Institute, which is where we know each other from. And before that he was a teacher and a stand up comedian.

<div class="episode-image_variable max-600">


![Doyne Farmer](images/michael.jpg)

</div>

We discuss:

- Whether you should stay in academia if you want to do impactful research
- How to start looking for roles at impact-driven research organisations
- What simple changes can improve how you write about your research
- The uses of 'reductionism' and quantitative thinking
- The concept of ‘reasoning transparency’
- Michael’s experience investigating nuclear security

## Recommendations

- [Interested in EA/longtermist research careers? Here are my top recommended resources](https://forum.effectivealtruism.org/posts/Na6pkfpZrfyKBhEcp/interested-in-ea-longtermist-research-careers-here-are-my)
- [AGI Safety Fundamentals](https://www.eacambridge.org/agi-safety-fundamentals) ([Governance Curriculum](https://www.eacambridge.org/ai-governance-curriculum))
- [Risks from Nuclear Weapons](https://forum.effectivealtruism.org/s/KJNrGbt3JWcYeifLk) — EA Forum
- Comedy
	- [Dylan Moran](https://dylanmoran.com/)
  - [Stewart Lee](https://www.stewartlee.co.uk/)
  - [Zach and Viggo](http://www.zachandviggo.com)

See also:

- Michael’s [ranked list of all EA-relevant (audio)books I've read](https://forum.effectivealtruism.org/posts/zCJDF6iNSJHnJ6Aq6/a-ranked-list-of-all-ea-relevant-audio-books-i-ve-read)

## Resources

### Expressions of interest forms for longtermist research

- [Expression of Interest — AI Governance and Strategy](https://rethinkpriorities.pinpointhq.com/en/jobs/51452)
- [Expression of Interest — General Longtermism](https://rethinkpriorities.pinpointhq.com/en/jobs/51712)
- [Expression of Interest — Longtermist Founder or Early Employee with Rethink Priorities](https://docs.google.com/forms/d/e/1FAIpQLSfqsmhmRuTJLiGt0J9KGu43E8t6ab6WT9d3C3r4XNkBtqJ6GA/viewform)

Relevant: Michael’s [list of pros and cons for working at Rethink Priorities](https://forum.effectivealtruism.org/posts/3vXXthjBKhNo8sgFv/job-ad-research-important-longtermist-topics-at-rethink?commentId=tLm5Cgy4GH5u8CfN5).

### Applying for research jobs and funding

All authored by Michael.

- [Don’t think, just apply! (usually)](https://forum.effectivealtruism.org/posts/Fahv9knHhPi6pWPEB/don-t-think-just-apply-usually)
- [List of EA funding opportunities](https://forum.effectivealtruism.org/posts/DqwxrdyQxcMQ8P2rD/list-of-ea-funding-opportunities)
- [Why YOU should consider applying for funding](https://docs.google.com/presentation/d/1awjq4A-262EzV8BY5IXxnDcaB5oBZCIf7rF4QUXV0T0/edit#slide=id.p)

### Organisations mentioned in the episode

- [Rethink Priorities](https://rethinkpriorities.org/)
- [Future of Humanity Institute](https://www.fhi.ox.ac.uk/)
- [Convergence Analysis](https://www.convergenceanalysis.org/)
- [EA Funds](https://funds.effectivealtruism.org/)

### Research skills and concepts

- [Notes on EA-related research, writing, testing fit, learning, and the Forum](https://forum.effectivealtruism.org/posts/J7PsetipHFoj2Mv7R/notes-on-ea-related-research-writing-testing-fit-learning) by Michael Aird
- [Suggestion: EAs should post more summaries and collections](https://forum.effectivealtruism.org/posts/6trt8mTsKfqJJbfJa/suggestion-eas-should-post-more-summaries-and-collections) by Michael Aird
- [Building a Theory of Change for Your Research](https://docs.google.com/presentation/d/18KgEmezuyKeg0Ha_Fvmpbk8rRevmlEiuqJnV5FJrfHE/edit#slide=id.gedf36733a6_0_0) by Michael Aird
- [Reasoning Transparency](https://www.openphilanthropy.org/research/reasoning-transparency/) by Luke Muehlhauser
- [Useful Vices for Wicked Problems](https://www.cold-takes.com/useful-vices-for-wicked-problems/) by Holden Karnofsky
- [Learning By Writing](https://www.cold-takes.com/learning-by-writing/) by Holden Karnofsky
- [Independent impressions](https://forum.effectivealtruism.org/posts/2WS3i7eY4CdLH99eg/independent-impressions) by Michael
- Michael’s [Tips & readings on getting useful input from busy people](https://docs.google.com/document/u/1/d/1Yt5eJYr9gMvn_cqaBZdOUlnRF7QQlYIAzSdSplGgrkA/edit) 

### General writing skills

- Michael’s [Tips & readings on good writing/communication](https://docs.google.com/document/d/1KVyOGE7JU8A0YkHZFsKZVSrlLJSWqfwcddq0cPCqUY0/edit#)
- The [curse of knowledge](https://en.wikipedia.org/wiki/Curse_of_knowledge)

Not mentioned my Michael but still maybe useful —

- [On Writing Well](https://www.goodreads.com/book/show/53343.On_Writing_Well) by William Zinsser
- [The Sense of Style](https://www.goodreads.com/en/book/show/20821371) by Steven Pinker

### Nuclear risk

- [Nuclear risk research ideas: Summary & introduction](https://forum.effectivealtruism.org/posts/N4oG4jMZRkbaKuPZF/nuclear-risk-research-ideas-summary-and-introduction)
- [8 possible high-level goals for work on nuclear risk](https://forum.effectivealtruism.org/posts/dASEFCurRpNot4Gpc/8-possible-high-level-goals-for-work-on-nuclear-risk)
- Some of [Michael’s other posts](https://forum.effectivealtruism.org/users/michaela) on the EA Forum

### AI governance

- [The longtermist AI governance landscape: a basic overview](https://forum.effectivealtruism.org/posts/ydpo7LcJWhrr2GJrx/the-longtermist-ai-governance-landscape-a-basic-overview) by Sam Clarke (not mentioned during the episode, but suggested by Michael afterwards)
- [Some AI Governance Research Ideas](https://docs.google.com/document/d/13LJhP3ksrcEBKxYFG5GkJaC2UoxHKUYAHCRdRlpePEc/edit#)**,** compiled by Markus Anderljung & Alexis Carlier
- Strategic Perspectives on Long-term AI Governance by Matthijs Maas 
- Some of [Michael’s posts](https://forum.effectivealtruism.org/users/michaela) on the EA Forum

### Other research topic ideas

- [A central directory for open research questions](https://forum.effectivealtruism.org/posts/MsNpJBzv5YhdfNHc9/a-central-directory-for-open-research-questions)
- [Some history topics it might be very valuable to investigate](https://forum.effectivealtruism.org/posts/psKZNMzCyXybcoEZR/some-history-topics-it-might-be-very-valuable-to-investigate)
- [Crucial questions for longtermists](https://forum.effectivealtruism.org/posts/wicAtfihz2JmPRgez/crucial-questions-for-longtermists)

### Miscellaneous

- [Space Governance](https://80000hours.org/problem-profiles/space-governance/) — 80,000 Hours

## Transcript

*Coming soon!*
