---
number: "60"
path: "/episodes/sevilla"
date: "2023-03-15"
title: "Jaime Sevilla on Trends in Machine Learning"
audio: "https://pinecast.com/listen/f1c71858-eeee-464c-a1bf-2e6ef79272e5.mp3"
featuredImage: "images/jaime-share.png"
backgroundImage: "images/jaime-bg.png"
apple: ""
spotify: ""
google: ""
status: "live"
---

Jaime Sevilla is the Director of [Epoch](https://epochai.org/), a team of researchers investigating and forecasting the development of advanced AI. This is his second time on the podcast.

<div class="episode-image_variable">

![Jaime Sevilla](images/jaime.jpg)

</div>

Over the next few episodes, we will be exploring the potential for [catastrophe cause by advanced artificial intelligence](https://80000hours.org/problem-profiles/artificial-intelligence/). Why? First, you might think that AI is likely to become transformatively powerful within our lifetimes. Second, you might think that such transformative AI could result in catastrophe unless weâ€™re very careful about how it gets implemented.

This episode is about understanding the first of those two claims.

Fin spoke with Jaime about:

- We've seen a crazy amount of progress in AI capabilities in the last few months, with AI models like [ChatGPT](https://openai.com/blog/chatgpt), [GPT-4](https://openai.com/research/gpt-4), Microsoft's [Bing chatbot](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/), and more. How should we think about that progress continuing into the future?
- How has the [amount of compute](https://epochai.org/blog/compute-trends) used to train AI models been changing over time?
- What about [algorithmic](https://epochai.org/blog/revisiting-algorithmic-progress) [efficiency](https://epochai.org/blog/revisiting-algorithmic-progress)?
- How expensive are the [training runs](https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems) for big ML models like GTP-3 and 4?
- Will we soon [run out of data](https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset) to keep making progress in training big ML models?
- How many words is a typical human trained on, compared to LLMs like ChatGPT?
- What will become of AI-generated art?

## Further reading

- [Epoch's website](https://epochai.org/)
- [Jaime's Twitter](https://twitter.com/Jsevillamol)
- Jaime's [AI-generated art project](https://www.connectomeart.com/)
- Epoch reports and blog posts that we discuss in the interview
  - [Will we run out of ML data? Evidence from projecting dataset size trends](https://epochai.org/blog/will-we-run-out-of-ml-data-evidence-from-projecting-dataset)
  - [Compute Trends Across Three Eras of Machine Learning](https://epochai.org/blog/compute-trends)
  - [Revisiting algorithmic progress](https://epochai.org/blog/revisiting-algorithmic-progress)
  - [Literature review of Transformative Artificial Intelligence timelines](https://epochai.org/blog/literature-review-of-transformative-artificial-intelligence-timelines)
  - [Predicting GPU performance](https://epochai.org/blog/predicting-gpu-performance)

## Transcript

*Coming soon!*
