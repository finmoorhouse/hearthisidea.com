---
number: "63"
path: "/episodes/garfinkel"
date: "2023-05-13"
title: "Ben Garfinkel on AI Governance"
audio: "https://pinecast.com/listen/38d8b847-e34e-49f5-b041-6e9306943665.mp3"
featuredImage: "images/ben-share.png"
backgroundImage: "images/bg2.png"
apple: ""
spotify: ""
google: ""
status: "live"
---

[Ben Garfinkel](https://www.benmgarfinkel.com/) is a Research Fellow at the the University of Oxford and Acting Director of the [Centre for the Governance of AI](https://www.governance.ai/about-us).

![Ben Garfinkel](images/ben.jpg)

In this episode we talk about:
* An overview of AI governance space
  * Disentangling concrete research questions that Ben would like to see more work on  
* Seeing how existing arguments for the risks from transformative AI have held up and Ben’s personal motivations for working on global risks from AI
* GovAI’s own work and opportunities for listeners to get involved

## Ben's recommended reading
* [AGI Safety Fundamentals' AI Governance Course](https://www.agisafetyfundamentals.com/ai-governance-curriculum)
* [Joe Carlsmith's Existential Risk from Power-Seeking AI]()
* [“Is power-seeking AI an existential risk?”](https://joecarlsmith.com/2023/03/22/existential-risk-from-power-seeking-ai-shorter-version) by Joseph Carlsmith (shorter version)
  * [Ben's review of](https://docs.google.com/document/d/1FlGPHU3UtBRj4mBPkEZyBQmAuZXnyvHU-yaH-TiNt8w/edit?usp=sharing) Carlsmith's report
## Further reading
* AI Governance
  * [Center for the Governance of AI](https://www.governance.ai/) and its [research agenda](http://www.governance.ai/agenda)
  * [Cold Takes' Transformative AI issues (not just misalignment): an overview](https://www.cold-takes.com/transformative-ai-issues-not-just-misalignment-an-overview/)
  * [Lennart Stern’s compute governance reading list](https://docs.google.com/document/d/1DF31DIkwS9GONzmy1W3nuI9HRAwSKy8JcIbzKYXg-ic/edit?usp=sharing))
  * [Mauricio Baker's takeaways on US policy careers](https://forum.effectivealtruism.org/posts/z9hzAB9mfgcpXmcze/takeaways-on-us-policy-careers-part-1-paths-to-impact-and)”
* Previous historic analogies to AGI
  * Transformative periods in history
  	* [The Neolithic Revolution](https://en.wikipedia.org/wiki/Neolithic_Revolution). See also [Against the Grain](https://www.goodreads.com/book/show/34324534-against-the-grain?from_search=true&from_srp=true&qid=P7SXrvkMG3&rank=1)
  	* [The Industrial Revolution](https://en.wikipedia.org/wiki/Industrial_Revolution). See also our [interview with Victoria Bateman](https://hearthisidea.com/episodes/victoria)
  	* The [Offense-Defense Balance](https://www.belfercenter.org/publication/what-offense-defense-balance-and-how-can-we-measure-it) (and [how it might apply to publishing AI research](https://www.governance.ai/research-paper/the-offense-defense-balance-of-scientific-knowledge-does-publishing-ai-research-reduce-misuse))
  * Moments of possible crises
  	* The Cuban Missile Crisis. See also [work by Thomas Shelling](https://www.belfercenter.org/publication/preventing-nuclear-war-schellings-strategies)
  	* [Racing Through a Minefield: the AI deployment problem](https://www.cold-takes.com/racing-through-a-minefield-the-ai-deployment-problem/)
  * Race dynamics
  	* [Microsoft Bing's and Alphabet's Google race on chatbots](https://www.washingtonpost.com/technology/2023/04/19/google-bard-pichai-60-minutes-ai/)
  	* [Are you really in a race? The Cautionary Tales of Szilárd and Ellsberg](https://forum.effectivealtruism.org/posts/cXBznkfoPJAjacFoT/are-you-really-in-a-race-the-cautionary-tales-of-szilard-and)
* On the risks of AI
  * Range of beliefs
  	* [2022 Expert Survey on Progress in AI](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/)
  	* [Preliminary survey results: US and European publics overwhelmingly and increasingly agree that AI needs to be managed carefully](https://www.governance.ai/post/increasing-consensus-ai-requires-careful-management)
  	* [Alignment researchers disagree a lot](https://www.planned-obsolescence.org/disagreement-in-alignment/)
  * Efficient Market Hypothesis
  	* [AGI and the EMH: markets are not expecting aligned or unaligned AI in the next 30 years](https://forum.effectivealtruism.org/posts/8c7LycgtkypkgYjZx/agi-and-the-emh-markets-are-not-expecting-aligned-or)
  	* [No, the EMH does not imply that markets have long AGI timelines](https://forum.effectivealtruism.org/posts/Go5CDwyna3hAfngKP/no-the-emh-does-not-imply-that-markets-have-long-agi) and Carl Schulman's [comment](https://forum.effectivealtruism.org/posts/Go5CDwyna3hAfngKP/no-the-emh-does-not-imply-that-markets-have-long-agi?commentId=dxpcmdyoxZAsGnbii)
  * Topics were people have revised down the chance of existential risk
  	* [Nuclear winner](https://forum.effectivealtruism.org/posts/pMsnCieusmYqGW26W/how-bad-would-nuclear-winter-caused-by-a-us-russia-nuclear)
  	* [Climate change](https://forum.effectivealtruism.org/posts/BvNxD66sLeAT8u9Lv/climate-change-and-longtermism-new-book-length-report)
  	* [What if all the bees die?](https://www.nrdc.org/stories/world-without-bees-heres-what-happens-if-bees-go-extinct)
  * Arguments for x-risk
  	* [Nick Bostrom's Superintelligence](https://www.goodreads.com/book/show/106592758-super-intelligence?from_search=true&from_srp=true&qid=FVnb8QUcP6&rank=9)  and [Ben Garkfinkel's previous interview on the 80K podcast](https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/#where-does-this-leave-us-021519)
  	* [Cold Takes' How we could stumble into AI catastrophe](https://www.cold-takes.com/how-we-could-stumble-into-ai-catastrophe/)

## Transcript

*Coming soon!*

![Ben Garfinkel](images/bg3.png)
