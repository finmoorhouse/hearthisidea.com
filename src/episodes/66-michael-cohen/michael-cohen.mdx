---
number: "66"
path: "/episodes/Cohen"
date: "2023-06-10"
title: "Michael Cohen on Input Tampering in Advanced RL Agents"
audio: "https://pinecast.com/listen/8ae8d664-64de-46d3-92ed-a95500ef281e.mp3"
featuredImage: "images/cohen-share.png"
backgroundImage: "images/2.jpg"
apple: ""
spotify: ""
google: ""
status: "live"
---

[Michael Cohen](https://www.michael-k-cohen.com) is is a DPhil student at the University of Oxford with [Mike Osborne](https://www.robots.ox.ac.uk/~mosb/). He will be starting a postdoc with [Professor Stuart Russell](https://www.wikiwand.com/en/Stuart_J._Russell) at UC Berkeley, with the [Center for Human-Compatible AI](https://humancompatible.ai/). His research considers the expected behaviour of generally intelligent artificial agents, with a view to designing agents that we can expect to behave safely.

<div class="episode-image_variable episode-image_smaller">

![Katja Grace](images/michael.jpg)

</div>

In this episode we talk about:
* What is reinforcement learning, and how is it different from supervised and unsupervised learning?
* Michael's recently co-authored paper titled ['Advanced artificial agents intervene in the provision of reward'](https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064)
* Why it's hard to convey what we really want to RL learners â€” even when we know exactly what we want
* Reasons why advanced RL systems might tamper with their sources of input, and how this could go badly wrong
* What assumptions need to hold for this "input tampering" outcome, and whether they're likely
* How we could build even very advanced RL systems which avoid this failure mode
  * Through building "myopic" RL agents
  * Through building "risk averse" RL agents
  * Through "imitation learning"
  * Through "inverse reinforcement learning" (IRL)
* Is reward [really the optimisation target](https://www.lesswrong.com/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target)? Do models ["get reward"](https://www.lesswrong.com/posts/TWorNr22hhYegE4RT/models-don-t-get-reward)?
* Is the analogy between RL systems and evolution strong enough to show this isn't a worry?
* Why *[Survivor](https://www.wikiwand.com/en/Survivor_(American_TV_series))* is the best game show, and whether raising a baby is like training an RL system

![A figure from Michael's paper](images/fig1.jpg)


## Michael's recommended reading (and watching)
* ['RAMBO-RL: Robust Adversarial Model-Based Offline Reinforcement Learning'](https://arxiv.org/abs/2204.12581) by Marc Rigter, Bruno Lacerda, and Nick Hawes
* ['Quantilizers: A Safer Alternative to Maximizers for Limited Optimization'](https://intelligence.org/files/QuantilizersSaferAlternative.pdf) by Jessica Taylor
* [Season 40 of *Survivor*](https://www.wikiwand.com/en/Survivor:_Winners_at_War)
## Further reading

- Michael's [personal website](https://www.michael-k-cohen.com/)
- ['Advanced artificial agents intervene in the provision of reward'](https://onlinelibrary.wiley.com/doi/10.1002/aaai.12064) by [Michael K. Cohen](https://onlinelibrary.wiley.com/authored-by/Cohen/Michael+K.), [Marcus Hutter](https://onlinelibrary.wiley.com/authored-by/Hutter/Marcus), and [Michael A. Osborne](https://onlinelibrary.wiley.com/authored-by/Osborne/Michael+A.)
- ['Pessimism About Unknown Unknowns Inspires Conservatism'](https://www.learningtheory.org/colt2020/virtual/papers/paper_221.html) by Michael Cohen and Marcus Hutter
- ['Intelligence and Unambitiousness Using Algorithmic Information Theory'](https://ieeexplore.ieee.org/document/9409939) by Michael Cohen, Badri Vallambi, and Marcus Hutter
- ['Quantilizers: A Safer Alternative to Maximizers for Limited Optimization'](https://intelligence.org/files/QuantilizersSaferAlternative.pdf) by Jessica Taylor

## Transcript

*Coming soon!*

